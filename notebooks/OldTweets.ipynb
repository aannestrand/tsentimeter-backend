{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions:\n",
    "\n",
    "Most of the imported libraries can be just pip installed, but for fasttext, do the following:\n",
    "\n",
    "Install Microsoft Build Tools from here: https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "\n",
    "And then do the following in conda command promt window:\n",
    "\n",
    "git clone https://github.com/facebookresearch/fastText.git\n",
    "cd fastText\n",
    "pip install .\n",
    "\n",
    "Let me know if there are other issues!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import WordPunctTokenizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import emoji\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline\n",
    "import GetOldTweets3 as got\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import tweepy\n",
    "from pymongo import MongoClient\n",
    "import dns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model, you will need my model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.load_model(\"model_fasttext.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_mongo ():\n",
    "    client = MongoClient(\"mongodb+srv://ee461l-blog:trapdungeon@cluster0-1mz2k.mongodb.net/test?retryWrites=true&w=majority\")\n",
    "    db = client['tsentimeter']\n",
    "    return db\n",
    "\n",
    "db = connect_mongo()\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_collection = db.tweets\n",
    "tweets_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_of_positive_sentiment_from_tweet(tweet):\n",
    "    tweet = BeautifulSoup(tweet).get_text()\n",
    "    tweet = tweet.replace('\\n', ' ')\n",
    "    tweet = tweet.replace('\\x92',\"'\")\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", tweet).split())\n",
    "    tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    tweet = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", tweet).split())\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    CONTRACTIONS = load_dict_contractions()\n",
    "    tweet = tweet.replace(\"’\",\"'\")\n",
    "    words = tweet.split()\n",
    "    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "    \n",
    "    tweet = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))\n",
    "    \n",
    "    SMILEY = load_dict_smileys()  \n",
    "    words = tweet.split()\n",
    "    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "    \n",
    "    tweet = emoji.demojize(tweet)\n",
    "    tweet = tweet.replace(\":\",\" \")\n",
    "    tweet = ' '.join(tweet.split())\n",
    "    \n",
    "    \n",
    "    pred = model.predict(tweet)\n",
    "    label, proba = pred\n",
    "    if proba[0] > 1:\n",
    "        proba[0] = 1\n",
    "    if proba[0] < 0:\n",
    "        proba[0] = 0\n",
    "    \n",
    "    if label[0] == \"__label__NEGATIVE\":\n",
    "        return (1-proba[0])\n",
    "        \n",
    "    else:\n",
    "        return (proba[0])   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify search parameters and create all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = 'Amazon' #keyword search. can also search by user with this API, but for now this is good enough\n",
    "since_date = '2020-01-01' #start date of searching\n",
    "until_date = '2020-02-20' #end date of searching (it will not inculde this day)\n",
    "count = 500 #number of tweets per request, but the way I have written this, it is number of tweets per day of searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting all the tweets, will take a while if over long time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.DataFrame(columns = ['date', 'text'])\n",
    "count_days = 0\n",
    "while (since_date!=until_date):\n",
    "    count_days = 0;\n",
    "    while (count_days < 8): #will get tweets in 10 day batches\n",
    "        count_days = count_days + 1\n",
    "        next_date = (datetime.strptime(since_date, '%Y-%m-%d') + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        print (next_date)\n",
    "        tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query).setSince(since_date).setUntil(next_date).setMaxTweets(count).setTopTweets(True)\n",
    "\n",
    "        # list that contains all tweets\n",
    "        tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "        # list of chosen tweet data\n",
    "        text_tweets = [[tweet.date, tweet.text, tweet.id, tweet.retweets, tweet.favorites] for tweet in tweets]\n",
    "        df_tweets = pd.concat([df_tweets, pd.DataFrame(text_tweets)], ignore_index=True)\n",
    "        \n",
    "        since_date = (datetime.strptime(since_date, '%Y-%m-%d') + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        if (since_date==until_date):\n",
    "            break;\n",
    "    \n",
    "    if(since_date!=until_date):\n",
    "        print(\"sleep\")\n",
    "        time.sleep(120) #2 minute delay since twitter has a rate limit on number of requests from this API\n",
    "        print(\"wake up\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_f = df_tweets.drop(['date', 'text'], axis = 1)\n",
    "df_tweets_f = df_tweets_f.rename(columns = {0:'date', 1:'text', 2:'id', 3:'retweets', 4:'favorites'})\n",
    "\n",
    "#format dates from datetime to string objects\n",
    "date_string = []\n",
    "for row in df_tweets_f.iterrows():\n",
    "    date_string.append((row[1].date).strftime('%a %b %d %H:%M:%S %z %Y'))\n",
    "df_tweets_f['date_string'] = pd.DataFrame(date_string)\n",
    "df_tweets_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up all the tweets and getting the labels as well as a df for evaluation of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets=[]\n",
    "for row in df_tweets_f.iterrows():\n",
    "                  \n",
    "    tweet = BeautifulSoup(row[1].text).get_text()\n",
    "    tweet = tweet.replace('\\n', ' ')\n",
    "    tweet = tweet.replace('\\x92',\"'\")\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", tweet).split())\n",
    "    tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    tweet = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", tweet).split())\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    CONTRACTIONS = load_dict_contractions()\n",
    "    tweet = tweet.replace(\"’\",\"'\")\n",
    "    words = tweet.split()\n",
    "    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "    \n",
    "    tweet = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))\n",
    "    \n",
    "    SMILEY = load_dict_smileys()  \n",
    "    words = tweet.split()\n",
    "    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "    \n",
    "    tweet = emoji.demojize(tweet)\n",
    "    tweet = tweet.replace(\":\",\" \")\n",
    "    tweet = ' '.join(tweet.split())\n",
    "    clean_tweets.append(tweet)\n",
    "    \n",
    "clean_tweets = pd.DataFrame(clean_tweets)\n",
    "df_tweets_f['processed_tweets'] = clean_tweets #all formatted tweets are now in df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_for_df = []\n",
    "proba_for_df = []\n",
    "\n",
    "for row in df_tweets_f.iterrows():\n",
    "\n",
    "    \n",
    "    pred = model.predict(row[1].processed_tweets)\n",
    "    label, proba = pred\n",
    "    if proba[0] > 1:\n",
    "        proba[0] = 1\n",
    "    if proba[0] < 0:\n",
    "        proba[0] = 0\n",
    "    \n",
    "    if label[0] == \"__label__NEGATIVE\":\n",
    "        labels_for_df.append('negative')\n",
    "        proba_for_df.append(1-proba[0])\n",
    "        \n",
    "    else:\n",
    "        labels_for_df.append('positive')\n",
    "        proba_for_df.append(proba[0])\n",
    "\n",
    "\n",
    "\n",
    "df_tweets_f['sentiment'] = pd.DataFrame(labels_for_df)\n",
    "df_tweets_f['pred_proba'] = pd.DataFrame(proba_for_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_f.date_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_tweets_f.iterrows():\n",
    "    db_tweet = {}\n",
    "\n",
    "    db_tweet['topic'] = text_query\n",
    "    db_tweet['search_method'] = 'keyword'\n",
    "    db_tweet['date'] = str(row[1].date_string)\n",
    "    db_tweet['tweet_id'] = str(row[1].id)\n",
    "    db_tweet['tweet'] = row[1].text\n",
    "    db_tweet['retweet_count'] = row[1].retweets\n",
    "    db_tweet['favorite_count'] = row[1].favorites\n",
    "    db_tweet['sentiment'] = str(row[1].pred_proba)\n",
    "\n",
    "    tweets_collection.insert_one(db_tweet)\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tweets_collection.delete_many({\"topic\": \"Tesla\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\"topic\" : \"quarantine\"}\n",
    "replacement_data = {\"topic\" : \"Quarantine\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_collection.find().forEach(function(doc) {\n",
    "    doc.tweet.topic = doc.tweet.topic.replace('quarantine', 'Quarantine');\n",
    "    tweets_collection.save(doc);\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_smileys():\n",
    "    \n",
    "    return {\n",
    "        \":‑)\":\"smiley\",\n",
    "        \":-]\":\"smiley\",\n",
    "        \":-3\":\"smiley\",\n",
    "        \":->\":\"smiley\",\n",
    "        \"8-)\":\"smiley\",\n",
    "        \":-}\":\"smiley\",\n",
    "        \":)\":\"smiley\",\n",
    "        \":]\":\"smiley\",\n",
    "        \":3\":\"smiley\",\n",
    "        \":>\":\"smiley\",\n",
    "        \"8)\":\"smiley\",\n",
    "        \":}\":\"smiley\",\n",
    "        \":o)\":\"smiley\",\n",
    "        \":c)\":\"smiley\",\n",
    "        \":^)\":\"smiley\",\n",
    "        \"=]\":\"smiley\",\n",
    "        \"=)\":\"smiley\",\n",
    "        \":-))\":\"smiley\",\n",
    "        \":‑D\":\"smiley\",\n",
    "        \"8‑D\":\"smiley\",\n",
    "        \"x‑D\":\"smiley\",\n",
    "        \"X‑D\":\"smiley\",\n",
    "        \":D\":\"smiley\",\n",
    "        \"8D\":\"smiley\",\n",
    "        \"xD\":\"smiley\",\n",
    "        \"XD\":\"smiley\",\n",
    "        \":‑(\":\"sad\",\n",
    "        \":‑c\":\"sad\",\n",
    "        \":‑<\":\"sad\",\n",
    "        \":‑[\":\"sad\",\n",
    "        \":(\":\"sad\",\n",
    "        \":c\":\"sad\",\n",
    "        \":<\":\"sad\",\n",
    "        \":[\":\"sad\",\n",
    "        \":-||\":\"sad\",\n",
    "        \">:[\":\"sad\",\n",
    "        \":{\":\"sad\",\n",
    "        \":@\":\"sad\",\n",
    "        \">:(\":\"sad\",\n",
    "        \":'‑(\":\"sad\",\n",
    "        \":'(\":\"sad\",\n",
    "        \":‑P\":\"playful\",\n",
    "        \"X‑P\":\"playful\",\n",
    "        \"x‑p\":\"playful\",\n",
    "        \":‑p\":\"playful\",\n",
    "        \":‑Þ\":\"playful\",\n",
    "        \":‑þ\":\"playful\",\n",
    "        \":‑b\":\"playful\",\n",
    "        \":P\":\"playful\",\n",
    "        \"XP\":\"playful\",\n",
    "        \"xp\":\"playful\",\n",
    "        \":p\":\"playful\",\n",
    "        \":Þ\":\"playful\",\n",
    "        \":þ\":\"playful\",\n",
    "        \":b\":\"playful\",\n",
    "        \"<3\":\"love\"\n",
    "        }\n",
    "\n",
    "# self defined contractions\n",
    "def load_dict_contractions():\n",
    "    \n",
    "    return {\n",
    "        \"ain't\":\"is not\",\n",
    "        \"amn't\":\"am not\",\n",
    "        \"aren't\":\"are not\",\n",
    "        \"can't\":\"cannot\",\n",
    "        \"'cause\":\"because\",\n",
    "        \"couldn't\":\"could not\",\n",
    "        \"couldn't've\":\"could not have\",\n",
    "        \"could've\":\"could have\",\n",
    "        \"daren't\":\"dare not\",\n",
    "        \"daresn't\":\"dare not\",\n",
    "        \"dasn't\":\"dare not\",\n",
    "        \"didn't\":\"did not\",\n",
    "        \"doesn't\":\"does not\",\n",
    "        \"don't\":\"do not\",\n",
    "        \"e'er\":\"ever\",\n",
    "        \"em\":\"them\",\n",
    "        \"everyone's\":\"everyone is\",\n",
    "        \"finna\":\"fixing to\",\n",
    "        \"gimme\":\"give me\",\n",
    "        \"gonna\":\"going to\",\n",
    "        \"gon't\":\"go not\",\n",
    "        \"gotta\":\"got to\",\n",
    "        \"hadn't\":\"had not\",\n",
    "        \"hasn't\":\"has not\",\n",
    "        \"haven't\":\"have not\",\n",
    "        \"he'd\":\"he would\",\n",
    "        \"he'll\":\"he will\",\n",
    "        \"he's\":\"he is\",\n",
    "        \"he've\":\"he have\",\n",
    "        \"how'd\":\"how would\",\n",
    "        \"how'll\":\"how will\",\n",
    "        \"how're\":\"how are\",\n",
    "        \"how's\":\"how is\",\n",
    "        \"I'd\":\"I would\",\n",
    "        \"I'll\":\"I will\",\n",
    "        \"I'm\":\"I am\",\n",
    "        \"I'm'a\":\"I am about to\",\n",
    "        \"I'm'o\":\"I am going to\",\n",
    "        \"isn't\":\"is not\",\n",
    "        \"it'd\":\"it would\",\n",
    "        \"it'll\":\"it will\",\n",
    "        \"it's\":\"it is\",\n",
    "        \"I've\":\"I have\",\n",
    "        \"kinda\":\"kind of\",\n",
    "        \"let's\":\"let us\",\n",
    "        \"mayn't\":\"may not\",\n",
    "        \"may've\":\"may have\",\n",
    "        \"mightn't\":\"might not\",\n",
    "        \"might've\":\"might have\",\n",
    "        \"mustn't\":\"must not\",\n",
    "        \"mustn't've\":\"must not have\",\n",
    "        \"must've\":\"must have\",\n",
    "        \"needn't\":\"need not\",\n",
    "        \"ne'er\":\"never\",\n",
    "        \"o'\":\"of\",\n",
    "        \"o'er\":\"over\",\n",
    "        \"ol'\":\"old\",\n",
    "        \"oughtn't\":\"ought not\",\n",
    "        \"shalln't\":\"shall not\",\n",
    "        \"shan't\":\"shall not\",\n",
    "        \"she'd\":\"she would\",\n",
    "        \"she'll\":\"she will\",\n",
    "        \"she's\":\"she is\",\n",
    "        \"shouldn't\":\"should not\",\n",
    "        \"shouldn't've\":\"should not have\",\n",
    "        \"should've\":\"should have\",\n",
    "        \"somebody's\":\"somebody is\",\n",
    "        \"someone's\":\"someone is\",\n",
    "        \"something's\":\"something is\",\n",
    "        \"that'd\":\"that would\",\n",
    "        \"that'll\":\"that will\",\n",
    "        \"that're\":\"that are\",\n",
    "        \"that's\":\"that is\",\n",
    "        \"there'd\":\"there would\",\n",
    "        \"there'll\":\"there will\",\n",
    "        \"there're\":\"there are\",\n",
    "        \"there's\":\"there is\",\n",
    "        \"these're\":\"these are\",\n",
    "        \"they'd\":\"they would\",\n",
    "        \"they'll\":\"they will\",\n",
    "        \"they're\":\"they are\",\n",
    "        \"they've\":\"they have\",\n",
    "        \"this's\":\"this is\",\n",
    "        \"those're\":\"those are\",\n",
    "        \"'tis\":\"it is\",\n",
    "        \"'twas\":\"it was\",\n",
    "        \"wanna\":\"want to\",\n",
    "        \"wasn't\":\"was not\",\n",
    "        \"we'd\":\"we would\",\n",
    "        \"we'd've\":\"we would have\",\n",
    "        \"we'll\":\"we will\",\n",
    "        \"we're\":\"we are\",\n",
    "        \"weren't\":\"were not\",\n",
    "        \"we've\":\"we have\",\n",
    "        \"what'd\":\"what did\",\n",
    "        \"what'll\":\"what will\",\n",
    "        \"what're\":\"what are\",\n",
    "        \"what's\":\"what is\",\n",
    "        \"what've\":\"what have\",\n",
    "        \"when's\":\"when is\",\n",
    "        \"where'd\":\"where did\",\n",
    "        \"where're\":\"where are\",\n",
    "        \"where's\":\"where is\",\n",
    "        \"where've\":\"where have\",\n",
    "        \"which's\":\"which is\",\n",
    "        \"who'd\":\"who would\",\n",
    "        \"who'd've\":\"who would have\",\n",
    "        \"who'll\":\"who will\",\n",
    "        \"who're\":\"who are\",\n",
    "        \"who's\":\"who is\",\n",
    "        \"who've\":\"who have\",\n",
    "        \"why'd\":\"why did\",\n",
    "        \"why're\":\"why are\",\n",
    "        \"why's\":\"why is\",\n",
    "        \"won't\":\"will not\",\n",
    "        \"wouldn't\":\"would not\",\n",
    "        \"would've\":\"would have\",\n",
    "        \"y'all\":\"you all\",\n",
    "        \"you'd\":\"you would\",\n",
    "        \"you'll\":\"you will\",\n",
    "        \"you're\":\"you are\",\n",
    "        \"you've\":\"you have\",\n",
    "        \"Whatcha\":\"What are you\",\n",
    "        \"luv\":\"love\",\n",
    "        \"sux\":\"sucks\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
